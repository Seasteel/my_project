{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_cnn_article(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    title = soup.find('h1').get_text() if soup.find('h1') else 'N/A'\n",
    "    author = soup.find('span', {\"class\": 'byline__name'}).get_text() if soup.find('span', {\"class\": 'byline__name'}) else 'N/A'\n",
    "    date = soup.find('div', {'class': 'timestamp'}).get_text() if soup.find('div', {'class': 'timestamp'}) else 'N/A'\n",
    "    body = ' '.join([p.get_text() for p in soup.find_all('div', {'class': 'article__content-container'})])\n",
    "    \n",
    "    return {\n",
    "        'title': title.strip(),\n",
    "        'author': author.strip(),\n",
    "        'date': date.strip(),\n",
    "        'body': body.strip(),\n",
    "        'url': url.strip()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://edition.cnn.com/business/tech'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "links = []\n",
    "for link in  soup.findAll('a', {'class': 'container__link container__link--type-article container_vertical-strip__link'}):\n",
    "    links.append(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = list(set(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_data = []\n",
    "for i in links:\n",
    "    url = f'https://edition.cnn.com{i}'\n",
    "    article_data = scrape_cnn_article(url)\n",
    "    all_articles_data.append(article_data)\n",
    "with open('cnn_articles.json', 'w') as f:\n",
    "    json.dump(all_articles_data, f,indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_articles.json', 'r') as file:\n",
    "    original_json = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(articles):\n",
    "    labels = set()\n",
    "    for article in articles:\n",
    "        words = re.findall(r'\\b\\w+\\b', article['title'])\n",
    "        for word in words:\n",
    "            labels.add(word)\n",
    "    return list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = extract_labels(original_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:982: UserWarning: Not enough free disk space to download the file. The expected file size is: 1629.44 MB. The target location C:\\Users\\HP\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli\\blobs only has 1287.78 MB free disk space.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:982: UserWarning: Not enough free disk space to download the file. The expected file size is: 1629.44 MB. The target location C:\\Users\\HP\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli\\blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model facebook/bart-large-mnli with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.bart.modeling_bart.BartForSequenceClassification'>). See the original errors:\n\nwhile loading with AutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 563, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 3351, in from_pretrained\n    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py\", line 399, in cached_file\n    resolved_file = hf_hub_download(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1221, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1367, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1884, in _download_to_tmp_and_move\n    http_get(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 542, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\nwhile loading with BartForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 3351, in from_pretrained\n    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py\", line 399, in cached_file\n    resolved_file = hf_hub_download(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1221, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1367, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1884, in _download_to_tmp_and_move\n    http_get(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 542, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\Vstudio_project\\Machine_Learning\\Web_scraping.ipynb Cell 10\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/Vstudio_project/Machine_Learning/Web_scraping.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m classifier \u001b[39m=\u001b[39m pipeline(\u001b[39m'\u001b[39;49m\u001b[39mzero-shot-classification\u001b[39;49m\u001b[39m'\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfacebook/bart-large-mnli\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\__init__.py:906\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    905\u001b[0m     model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m--> 906\u001b[0m     framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    907\u001b[0m         model,\n\u001b[0;32m    908\u001b[0m         model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m    909\u001b[0m         config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    910\u001b[0m         framework\u001b[39m=\u001b[39mframework,\n\u001b[0;32m    911\u001b[0m         task\u001b[39m=\u001b[39mtask,\n\u001b[0;32m    912\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    913\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    914\u001b[0m     )\n\u001b[0;32m    916\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    917\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:296\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[39mfor\u001b[39;00m class_name, trace \u001b[39min\u001b[39;00m all_traceback\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    295\u001b[0m             error \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwhile loading with \u001b[39m\u001b[39m{\u001b[39;00mclass_name\u001b[39m}\u001b[39;00m\u001b[39m, an error is thrown:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtrace\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 296\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    297\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not load model \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m with any of the following classes: \u001b[39m\u001b[39m{\u001b[39;00mclass_tuple\u001b[39m}\u001b[39;00m\u001b[39m. See the original errors:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00merror\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m         )\n\u001b[0;32m    300\u001b[0m \u001b[39mif\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     framework \u001b[39m=\u001b[39m infer_framework(model\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model facebook/bart-large-mnli with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.bart.modeling_bart.BartForSequenceClassification'>). See the original errors:\n\nwhile loading with AutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 563, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 3351, in from_pretrained\n    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py\", line 399, in cached_file\n    resolved_file = hf_hub_download(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1221, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1367, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1884, in _download_to_tmp_and_move\n    http_get(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 542, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\nwhile loading with BartForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 3351, in from_pretrained\n    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py\", line 399, in cached_file\n    resolved_file = hf_hub_download(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1221, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1367, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1884, in _download_to_tmp_and_move\n    http_get(\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 542, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\n\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_json = {\n",
    "    \"documents\": []\n",
    "}\n",
    "\n",
    "# Classify each article and populate the new JSON structure\n",
    "for article in original_json['articles']:\n",
    "    # Join the title and body to create a single text for classification\n",
    "    text = article['title'] + \" \" + \" \".join(article['body'])\n",
    "    \n",
    "    # Perform the classification\n",
    "    classification = classifier(text, candidate_labels)\n",
    "    \n",
    "    # Get the highest scoring category\n",
    "    topic = classification['labels'][0]\n",
    "    \n",
    "    # Create the concept entry\n",
    "    concept = {\n",
    "        \"concept\": article['title'],\n",
    "        \"content\": \" \".join(article['body'])  # Join all paragraphs into a single content string\n",
    "    }\n",
    "    \n",
    "    # Check if the topic already exists in new_json\n",
    "    topic_exists = False\n",
    "    for document in new_json[\"documents\"]:\n",
    "        if document[\"topic\"] == topic:\n",
    "            document[\"concepts\"].append(concept)\n",
    "            topic_exists = True\n",
    "            break\n",
    "    \n",
    "    # If the topic does not exist, create a new topic entry\n",
    "    if not topic_exists:\n",
    "        new_json[\"documents\"].append({\n",
    "            \"topic\": topic,\n",
    "            \"concepts\": [concept]\n",
    "        })\n",
    "\n",
    "# Convert the new structure to JSON\n",
    "new_json_str = json.dumps(new_json, indent=4)\n",
    "\n",
    "# Save the new JSON structure to a file\n",
    "with open('converted_cnn_articles.json', 'w') as file:\n",
    "    file.write(new_json_str)\n",
    "\n",
    "print(\"Conversion complete. New JSON structure saved to 'converted_cnn_articles.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "import re\n",
    "\n",
    "# Load the original JSON file\n",
    "with open('cnn_articles.json', 'r') as file:\n",
    "    original_json = json.load(file)\n",
    "\n",
    "# Extract candidate labels from article titles\n",
    "def extract_labels(articles):\n",
    "    labels = set()\n",
    "    for article in articles:\n",
    "        # Use simple regex to extract potential labels from titles\n",
    "        # Here, we assume words in titles might be good labels\n",
    "        words = re.findall(r'\\b\\w+\\b', article['title'])\n",
    "        for word in words:\n",
    "            labels.add(word)\n",
    "    return list(labels)\n",
    "\n",
    "# Extract candidate labels from the articles\n",
    "candidate_labels = extract_labels(original_json['articles'])\n",
    "\n",
    "# Initialize the text classification pipeline\n",
    "classifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\n",
    "\n",
    "# Create a new JSON structure\n",
    "new_json = {\n",
    "    \"documents\": []\n",
    "}\n",
    "\n",
    "# Classify each article and populate the new JSON structure\n",
    "for article in original_json['articles']:\n",
    "    # Join the title and body to create a single text for classification\n",
    "    text = article['title'] + \" \" + \" \".join(article['body'])\n",
    "    \n",
    "    # Perform the classification\n",
    "    classification = classifier(text, candidate_labels)\n",
    "    \n",
    "    # Get the highest scoring category\n",
    "    topic = classification['labels'][0]\n",
    "    \n",
    "    # Create the concept entry\n",
    "    concept = {\n",
    "        \"concept\": article['title'],\n",
    "        \"content\": \" \".join(article['body'])  # Join all paragraphs into a single content string\n",
    "    }\n",
    "    \n",
    "    # Check if the topic already exists in new_json\n",
    "    topic_exists = False\n",
    "    for document in new_json[\"documents\"]:\n",
    "        if document[\"topic\"] == topic:\n",
    "            document[\"concepts\"].append(concept)\n",
    "            topic_exists = True\n",
    "            break\n",
    "    \n",
    "    # If the topic does not exist, create a new topic entry\n",
    "    if not topic_exists:\n",
    "        new_json[\"documents\"].append({\n",
    "            \"topic\": topic,\n",
    "            \"concepts\": [concept]\n",
    "        })\n",
    "\n",
    "# Convert the new structure to JSON\n",
    "new_json_str = json.dumps(new_json, indent=4)\n",
    "\n",
    "# Save the new JSON structure to a file\n",
    "with open('converted_cnn_articles.json', 'w') as file:\n",
    "    file.write(new_json_str)\n",
    "\n",
    "print(\"Conversion complete. New JSON structure saved to 'converted_cnn_articles.json'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
